{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from io import BytesIO\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import openai\n",
    "import streamlit as st\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, OpenAI\n",
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:13:23.768 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "@st.cache_data\n",
    "def pdf_parser(file: BytesIO) -> List[str]:\n",
    "    \"\"\" Extract text from a pdf file object\n",
    "        Clean, remove specific symbols such as hyphenated word, fixing newlines\n",
    "        and return a list of string for a page of PDF\n",
    "    \"\"\"\n",
    "    pdf_reader = PdfReader(file)\n",
    "\n",
    "    output_str = []\n",
    "    for page in pdf_reader.pages():\n",
    "        txt = page.extract_text()                              # extract text from each page in the pdf file\n",
    "        txt = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", txt)           # Merge hyphenated words\n",
    "        txt = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", txt.strip()) # Fix newlines in the middles of sentences\n",
    "        txt = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", txt)                  # Remove multiple newlines\n",
    "        output_str.append(txt)\n",
    "\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 10:54:08.898 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "@st.cache_data\n",
    "def page_chunker(text: str) -> list[Document]:\n",
    "    \"\"\" Converts a list of strings to a list of LangChain Document objects.\n",
    "        Each Document represents a chunk of text of up to 4000 characters with metadata (configurable)\n",
    "    \"\"\"\n",
    "    if(text, str):\n",
    "        text = [text]\n",
    "    page_docs = [Document(page_content=page) for page in text] # Convert the text into a Document object\n",
    "\n",
    "    for i, doc in enumerate(page_docs):\n",
    "        doc.metadata[\"page\"] = i + 1                           # Add page numbers as metadata\n",
    "    \n",
    "    text_chunks = []\n",
    "    for doc in page_docs:\n",
    "        # Split text from the page\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=4000,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "            chunk_overlop=0\n",
    "            )\n",
    "        page_text = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "        # Convert page text into a Document object with metadata (page number & index)\n",
    "        for i, p_t in enumerate(page_text):\n",
    "            doc = Document(page_content=p_t, metadata={\"page\":doc.metadata[\"page\"], \"chunk\":i})\n",
    "            doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "            text_chunks.append(doc)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:17:26.102 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run e:\\Anaconda\\install3.7\\envs\\langchain\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "uploaded_file = st.file_uploader(\"Please Upload Your PDF File\", type=[\"pdf\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    name = uploaded_file.name\n",
    "    text = pdf_parser(uploaded_file)\n",
    "    pages = page_chunker(text)\n",
    "\n",
    "    if pages:\n",
    "        with st.expander(\"Show Page Content\", expanded=False):\n",
    "            page_sel = st.number_input(\n",
    "                label=\"Select Page\",\n",
    "                min_value=1,\n",
    "                max_value=len(pages),\n",
    "                step=1\n",
    "            )\n",
    "            pages[page_sel - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:02:03.109 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "@st.cache_data\n",
    "def text_embed(pages):\n",
    "    \"\"\" Embed and index the document using the FAISS vector store\n",
    "    \"\"\"\n",
    "    # Use langchain openAI embedding model\n",
    "    emb = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Indexing & save in a Vector DB\n",
    "    with st.spinner(\"It's indexing...\"):\n",
    "        index = FAISS.from_documents(pages, emb)\n",
    "    st.success(\"Embedding done\")\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
